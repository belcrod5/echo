//
//  SpeechRecognizerViewModel.swift
//  SiriTypeInput
//
//  2025/05/24 â†’ åŸºæœ¬å®Ÿè£…
//  2025/05/28 â†’ å¸¸æ™‚éŸ³å£°å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œ
//  2025/06/01 â†’ AudioEngine ã®å˜ä¸€ç”Ÿæˆ & å†åˆ©ç”¨ï¼å®‰å…¨ãª restart
//  2025/06/02 â†’ confirm / cancel / readback åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Œå…¨å¾©å…ƒ
//

import AVFoundation
import Speech
import SwiftUI
#if os(macOS)
import CoreAudio
import AppKit
#endif

#if DEBUG
private func _devLog(_ msg: String) { print(msg) }
#else
private func _devLog(_ msg: String) {}
#endif

@MainActor
final class SpeechRecognizerViewModel: NSObject, ObservableObject, SFSpeechRecognizerDelegate {

    // â”€â”€ UI State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @Published var transcript: String = "â€¦"
    @Published var phrases: [String] = []
    @Published var isListening = false

    // â”€â”€ Callbacks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    var debugLog: ((String) -> Void)?
    var onPhraseFinalized:   ((String) -> Void)?

    // â”€â”€ SpeechKit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    private let speechRecognizer = SFSpeechRecognizer(locale: .init(identifier: "ja-JP"))
    private let audioEngine = AVAudioEngine()                // ï¼‘å›ã ã‘ç”Ÿæˆ
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?

    // â”€â”€ Internal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    private var buildingPhrase = ""
    private var restartWorkItem:  DispatchWorkItem?
    private var activity:         NSObjectProtocol?
    private var hasFinalizedCurrentPhrase = false
    private var processingShouldStop      = false
    private var didPerformReadbackRecently = false
    private var isStartingRecording = false
    private var wantsToListen = false         // ãƒ¦ãƒ¼ã‚¶ãŒéŒ²éŸ³ã‚’æœ›ã‚“ã§ã„ã‚‹ã‹

    // â”€â”€ User-defaults Words â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    private var confirmPhrase: String { Self.loadPhrase(forKey: "ConfirmPhrase", defaultValue: "ç¢ºå®š") }
    private var cancelPhrase:  String { Self.loadPhrase(forKey: "CancelPhrase",  defaultValue: "ã‚­ãƒ£ãƒ³ã‚»ãƒ«") }
    private var readbackPhrase:String { Self.loadPhrase(forKey: "ReadbackPhrase",defaultValue: "ç¢ºèª") }
    private var exitPhrase:   String { Self.loadPhrase(forKey: "ExitPhrase",   defaultValue: "çµ‚äº†") }

    private static func loadPhrase(forKey key: String, defaultValue: String) -> String {
        let raw = UserDefaults.standard.string(forKey: key) ?? defaultValue
        return raw.replacingOccurrences(of: "\\n", with: "\n")
    }

    // â”€â”€ Init â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    override init() {
        super.init()
        speechRecognizer?.delegate = self
        configureAudioSession()
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(audioEngineConfigurationChanged(_:)),
            name: .AVAudioEngineConfigurationChange,
            object: nil)
    }

    // â”€â”€ Authorization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    func requestAuth(_ completion: @escaping (Bool) -> Void) {
        SFSpeechRecognizer.requestAuthorization { status in
            DispatchQueue.main.async { completion(status == .authorized) }
        }
    }

    // â”€â”€ Public Controls â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    func startRecording() {
        _devLog("â¡ï¸ startRecording() called. isRunning=\(audioEngine.isRunning) channels=\(audioEngine.inputNode.inputFormat(forBus: 0).channelCount) inputExists=\(hasAudioInputDevice)")

        guard !audioEngine.isRunning, !isStartingRecording else { return }

        isStartingRecording = true

        wantsToListen = true

        // ãƒã‚¤ã‚¯ãŒç„¡ã‘ã‚Œã°ä¸­æ–­
        guard hasAudioInputDevice else {
            debugLog?("âš ï¸ No audio input device â€“ recording aborted")
            _devLog("âš ï¸ No audio input device â€“ recording aborted (channels: \(audioEngine.inputNode.inputFormat(forBus: 0).channelCount))")
            isListening = false
            isStartingRecording = false
            return
        }

        prepareRecognitionRequest()

        let input = audioEngine.inputNode
        let fmt   = input.inputFormat(forBus: 0)
        input.removeTap(onBus: 0)
        input.installTap(onBus: 0, bufferSize: 4096, format: fmt) { [weak self] buf, _ in
            self?.recognitionRequest?.append(buf)
        }

        var engineStarted = false
        do {
            try audioEngine.start()
            engineStarted = true
            _devLog("ğŸ§ audioEngine started. input channels: \(audioEngine.inputNode.inputFormat(forBus: 0).channelCount)")
        } catch {
            _devLog("â—ï¸ audioEngine.start() failed: \(error.localizedDescription)")
            stopEngineCompletely()
            // Retry once after slight delay in case device is still initializing
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
                guard let self else { return }
                if self.wantsToListen && !self.audioEngine.isRunning {
                    self.startRecording()
                }
            }
        }

        guard engineStarted else { isStartingRecording = false; return }

        keepDeviceAwake()

        beginRecognition()

        isListening = true
        isStartingRecording = false
        transcript  = "â€¦"
        buildingPhrase = ""
        hasFinalizedCurrentPhrase = false
        processingShouldStop      = false
        didPerformReadbackRecently = false
    }

    func stopRecording(userInitiated: Bool = true) {
        if userInitiated { wantsToListen = false }
        isListening = false
        restartWorkItem?.cancel()
        tearDownRecognitionTask()
        stopEngineCompletely()
        buildingPhrase = ""
        if let act = activity { ProcessInfo.processInfo.endActivity(act); activity = nil }
    }

    // â”€â”€ Recognition Task â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    private func beginRecognition() {
        guard let req = recognitionRequest else { return }
        recognitionTask = speechRecognizer?.recognitionTask(with: req) { [weak self] res, err in
            guard let self else { return }
            if self.processingShouldStop { return }

            if let res       { self.handleRecognitionResult(res) }
            if let nserr = err as NSError? { self.handleRecognitionError(nserr) }
        }
    }

    // MARK: - å®Œå…¨å¾©å…ƒã—ãŸæ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ --------------------------

    private func handleRecognitionResult(_ res: SFSpeechRecognitionResult) {
        let rawText     = res.bestTranscription.formattedString          // æ”¹è¡Œã‚’å«ã‚€å…ƒãƒ†ã‚­ã‚¹ãƒˆ
        let cleanedText = rawText.replacingOccurrences(of: "\n", with: "") // æ”¹è¡Œé™¤å»ç‰ˆ

        if rawText != buildingPhrase {
            buildingPhrase = cleanedText
            transcript     = cleanedText
            debugLog?("ğŸ¤ transcript updated: \(cleanedText)")

            // â”€â”€ Cancel åˆ¤å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if (cancelPhrase == "\n" && rawText.contains("\n")) ||
               (cancelPhrase != "\n" && cleanedText.contains(cancelPhrase)) {
                cancelCurrentPhrase()
                processingShouldStop = true
                restart()
                return
            }

            // â”€â”€ Exit åˆ¤å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if (exitPhrase == "\n" && rawText.contains("\n")) ||
               (exitPhrase != "\n" && cleanedText.contains(exitPhrase)) {
                terminateApplication()
                return  // app will exit
            }

            // â”€â”€ Readback åˆ¤å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if !didPerformReadbackRecently {
                if (readbackPhrase == "\n" && rawText.contains("\n")) ||
                   (readbackPhrase != "\n" && cleanedText.contains(readbackPhrase)) {
                    readbackCurrentPhrase()
                    didPerformReadbackRecently = true
                    return
                }
            }

            // â”€â”€ Confirm åˆ¤å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if (confirmPhrase == "\n" && rawText.contains("\n")) ||
               (confirmPhrase != "\n" && cleanedText.contains(confirmPhrase)) {
                if !hasFinalizedCurrentPhrase {
                    finalizePhrase()
                    hasFinalizedCurrentPhrase = true
                }
                processingShouldStop = true
                restart()
                return
            }
        }

        // æœ€çµ‚çµæœãƒ•ãƒ©ã‚°
        if res.isFinal {
            if !hasFinalizedCurrentPhrase {
                finalizePhrase()
                hasFinalizedCurrentPhrase = true
            }
            processingShouldStop = true
            restart()
        }
    }

    private func handleRecognitionError(_ err: NSError) {
        let benign = [216, 209]                // ç„¡è¦–ã—ã¦ã‚ˆã„ã‚³ãƒ¼ãƒ‰
        if benign.contains(err.code) { return }

        // kAudioUnitErr_NoConnection (-10877) â†’ å…¥åŠ›çµŒè·¯ç„¡ã—ã€‚ãƒã‚¤ã‚¯æœªæ¥ç¶šæ™‚ã«å¤šç™ºã€‚
        if err.code == -10877 {
            _devLog("âš ï¸ Error -10877 (NoConnection) â€” stopping engine to avoid loop")
            stopRecording(userInitiated: false)
            return
        }

        let retryable = [1101, 1107, 1110, 89]
        if retryable.contains(err.code) || isListening {
            processingShouldStop = true
            restart()
        }
    }

    // â”€â”€ Phrase Handlingï¼ˆå…ƒå®Ÿè£…ã®ã¾ã¾ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    private func finalizePhrase() {
        let finalized = buildingPhrase.trimmingCharacters(in: .whitespaces)
        guard !finalized.isEmpty else { return }

        if finalized.contains(confirmPhrase) {
            let payload: String
            if confirmPhrase == "\n" {
                payload = finalized.replacingOccurrences(of: "\n", with: "")
            } else {
                payload = finalized.replacingOccurrences(of: confirmPhrase, with: "")
            }
            let trimmed = payload.trimmingCharacters(in: .whitespaces)
            if !trimmed.isEmpty {
                phrases.append(trimmed)
                onPhraseFinalized?(trimmed)
            }
        } else {
            phrases.append(finalized)
            onPhraseFinalized?(finalized)
        }
        buildingPhrase = ""
        transcript = ""
    }

    private func cancelCurrentPhrase() {
        buildingPhrase = ""
        transcript = "â€¦"
    }

    private func readbackCurrentPhrase() {
        let current = buildingPhrase.trimmingCharacters(in: .whitespaces)
        guard !current.isEmpty else { return }

        let textForTTS: String
        if readbackPhrase == "\n" {
            textForTTS = current.replacingOccurrences(of: "\n", with: "")
        } else {
            textForTTS = current.replacingOccurrences(of: readbackPhrase, with: "")
        }

        let cleaned = textForTTS.trimmingCharacters(in: .whitespaces)
        guard !cleaned.isEmpty else { return }

        let voiceID = UserDefaults.standard.string(forKey: "SelectedVoiceStyleId") ?? "1937616896"
        SpeechSpeakerViewModel.shared.speak(text: cleaned, id: voiceID)

        buildingPhrase = cleaned
        transcript     = cleaned
    }

    // â”€â”€ Restart â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    private func restart() {
        _devLog("ğŸ”„ restart() called. isListening: \(isListening), channels: \(audioEngine.inputNode.inputFormat(forBus: 0).channelCount), inputExists: \(hasAudioInputDevice)")
        processingShouldStop = true
        tearDownRecognitionTask()
        stopEngineCompletely()

        guard isListening else { return }

        // If no microphone is available, avoid entering a restart loop.
        guard hasAudioInputDevice else {
            debugLog?("â¸ï¸ Restart aborted â€“ no audio input device detected")
            isListening = false
            _devLog("â¸ï¸ Restart aborted â€“ no mic. Listening set to false.")
            if let act = activity {
                ProcessInfo.processInfo.endActivity(act)
                activity = nil
            }
            return
        }

        restartWorkItem?.cancel()
        let item = DispatchWorkItem { [weak self] in self?.startRecording() }
        restartWorkItem = item
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.3, execute: item)
    }

    // MARK: - Helpers ----------------------------------------------------------

    private func prepareRecognitionRequest() {
        tearDownRecognitionTask()
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        recognitionRequest?.shouldReportPartialResults = true
        recognitionRequest?.taskHint = .dictation
        recognitionRequest?.contextualStrings = [
            "ã‚ªãƒ¬å°‚ç”¨ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ", "ä¸‹ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«", "ä¸Šã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«",
            "Youtubeã§", "é€²ã‚€", "æˆ»ã‚‹", "æ›´æ–°",
            "ç•ªç›®ã‚’ã‚¯ãƒªãƒƒã‚¯", "ç•ªç›®ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹", "ã‚ºãƒ¼ãƒ ã‚¤ãƒ³", "ã‚ºãƒ¼ãƒ ã‚¢ã‚¦ãƒˆ",
            confirmPhrase, cancelPhrase, readbackPhrase, exitPhrase
        ]
    }

    private func tearDownRecognitionTask() {
        recognitionTask?.finish()
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest?.endAudio()
        recognitionRequest = nil
    }

    private func stopEngineCompletely() {
        if audioEngine.isRunning { audioEngine.stop() }
        audioEngine.inputNode.removeTap(onBus: 0)
        audioEngine.reset()
    }

    private func configureAudioSession() {
        #if !os(macOS)
        let s = AVAudioSession.sharedInstance()
        try? s.setCategory(.playAndRecord,
                           mode: .measurement,
                           options: [.defaultToSpeaker, .allowBluetooth])
        try? s.setPreferredSampleRate(48_000)
        try? s.setActive(true)
        #endif
    }

    private func keepDeviceAwake() {
        if activity == nil {
            let opts: ProcessInfo.ActivityOptions = [.userInitiated,
                                                     .idleSystemSleepDisabled,
                                                     .idleDisplaySleepDisabled]
            activity = ProcessInfo.processInfo.beginActivity(options: opts,
                                                             reason: "éŸ³å£°èªè­˜")
        }
    }

    // â”€â”€ Manual typing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    func typePhrase(_ text: String) {
        let txt = text.trimmingCharacters(in: .whitespaces)
        guard !txt.isEmpty else { return }
        phrases.append(txt)
        onPhraseFinalized?(txt)
    }

    // â”€â”€ Delegate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer,
                          availabilityDidChange available: Bool) {
        if !available { stopRecording() }
    }

    // MARK: - Input device availability ------------------------------------
    /// Checks if macOS reports a default audio input device (physical or virtual).
    #if os(macOS)
    private var hasSystemInputDevice: Bool {
        // AVFoundation: ç™ºè¦‹ã§ãã‚‹å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹æ•°
        let session = AVCaptureDevice.DiscoverySession(
            deviceTypes: [.builtInMicrophone, .externalUnknown],
            mediaType: .audio,
            position: .unspecified)
        let avCount = session.devices.count

        // AVFoundation ã§ã¯å…¨ãƒ‡ãƒã‚¤ã‚¹ãŒ .Microphone æ‰±ã„ã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€
        // CoreAudio ã® TransportType ã‚‚ä½µç”¨ã—ã¦ "æœ¬å½“ã«å†…è”µ" ã‹åˆ¤å®šã™ã‚‹ã€‚

        // CoreAudio: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ã®æœ‰ç„¡ã¨ TransportType ãŒ BuiltIn ã‹
        var deviceID = AudioDeviceID(0)
        var addr = AudioObjectPropertyAddress(
            mSelector: kAudioHardwarePropertyDefaultInputDevice,
            mScope: kAudioObjectPropertyScopeGlobal,
            mElement: kAudioObjectPropertyElementMain)
        var size = UInt32(MemoryLayout<AudioDeviceID>.size)
        let err = AudioObjectGetPropertyData(AudioObjectID(kAudioObjectSystemObject),
                                             &addr,
                                             0,
                                             nil,
                                             &size,
                                             &deviceID)

        let caAvailable = (err == noErr && deviceID != 0)

        var transportType: UInt32 = 0
        if err == noErr {
            var tSize = UInt32(MemoryLayout<UInt32>.size)
            var ttAddr = AudioObjectPropertyAddress(
                mSelector: kAudioDevicePropertyTransportType,
                mScope: kAudioObjectPropertyScopeGlobal,
                mElement: kAudioObjectPropertyElementMain)
            AudioObjectGetPropertyData(deviceID, &ttAddr, 0, nil, &tSize, &transportType)
        }

        // è¨±å¯ã—ãŸã„ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆå†…è”µ + Bluetooth + USBï¼‰
        let allowedTransports: Set<UInt32> = [
            kAudioDeviceTransportTypeBuiltIn,
            kAudioDeviceTransportTypeBluetooth,
            kAudioDeviceTransportTypeBluetoothLE,
            kAudioDeviceTransportTypeUSB
        ]

        let isAllowedTransport = allowedTransports.contains(transportType)

        // â”€â”€ Device name check (to skip dummy USB inputs like HDMI, Display etc.)
        var devName: String = ""
        if caAvailable {
            var nameAddr = AudioObjectPropertyAddress(
                mSelector: kAudioObjectPropertyName,
                mScope: kAudioObjectPropertyScopeGlobal,
                mElement: kAudioObjectPropertyElementMain)
            var cfName: CFString = "" as CFString
            var nameSize = UInt32(MemoryLayout<CFString>.size)
            let _ = AudioObjectGetPropertyData(deviceID, &nameAddr, 0, nil, &nameSize, &cfName)
            devName = cfName as String
        }

        let junkKeywords = ["hdmi", "display", "aggregate", "camera", "webcam", ") audio"]
        let isJunkDevice = junkKeywords.contains { devName.lowercased().contains($0) }

        let deviceAccepted = isAllowedTransport && !isJunkDevice

        _devLog("ğŸ” Mic detection â€“ AVFoundation devices: \(avCount), CA default present: \(caAvailable), transport=0x\(String(transportType, radix:16)) allowed=\(isAllowedTransport) devName=\(devName) junk=\(isJunkDevice)")

        return caAvailable && deviceAccepted
    }
    #endif

    /// Returns true if at least one usable audio input source is present.
    private var hasAudioInputDevice: Bool {
        #if os(macOS)
        return hasSystemInputDevice && (audioEngine.inputNode.inputFormat(forBus: 0).channelCount > 0)
        #else
        return AVAudioSession.sharedInstance().inputNumberOfChannels > 0
        #endif
    }

    @objc private func audioEngineConfigurationChanged(_ notification: Notification) {
        let channels = audioEngine.inputNode.inputFormat(forBus: 0).channelCount
        _devLog("âš™ï¸ audioEngineConfigurationChanged. channels: \(channels), inputExists: \(hasAudioInputDevice)")

        if !hasAudioInputDevice {
            // å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ãŒåˆ©ç”¨ä¸å¯ã«ãªã£ãŸ
            if isListening {
                debugLog?("ğŸ”Œ Audio input disappeared â€“ stopping recording")
                stopRecording(userInitiated: false)
            }
        } else {
            // å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ãŒåˆ©ç”¨å¯èƒ½ã«ãªã£ãŸ
            if wantsToListen && !audioEngine.isRunning {
                debugLog?("âœ… Audio input returned â€“ restarting recording")
                startRecording()
            }
        }
    }

    private func logAvailableAudioInputDevices() {
        #if os(macOS)
        let session = AVCaptureDevice.DiscoverySession(
            deviceTypes: [.builtInMicrophone, .externalUnknown],
            mediaType: .audio,
            position: .unspecified)
        _devLog("ğŸ“‹ Listing \(session.devices.count) audio input devices:")
        for d in session.devices {
            _devLog("   â€¢ \(d.localizedName) / type=\(d.deviceType.rawValue) / connected=\(d.isConnected)")
        }
        #endif
    }

    // MARK: - Application Termination -----------------------------
    private func terminateApplication() {
        #if os(macOS)
        NSApplication.shared.terminate(nil)
        #else
        exit(0)
        #endif
    }
}
